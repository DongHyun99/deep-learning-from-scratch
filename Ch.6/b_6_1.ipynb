{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. 학습관련 기술들  \r\n",
    "\r\n",
    "# 6.1 매개변수 갱신  \r\n",
    "\r\n",
    "최적화(optimization): 가능한 한 손실함수가 잔은 매개변수의 최적값을 찾는 것  \r\n",
    "매개변수의 공간이 매우 넓고 복잡하여 매우 어려움  \r\n",
    "따라서 미분을 이용한 확률적 경사하강법 (SGD)를 많이 사용  \r\n",
    "앞으로는 다른 최적화 기법에 대해서도 다룰 것이다.  \r\n",
    "\r\n",
    "## 6.1.1 모험가 이야기  \r\n",
    "\r\n",
    "## 6.1.2 확률적 경사 하강법(SGD)  \r\n",
    "\r\n",
    "SGD를 수식으로 쓰면 다음과 같다.  \r\n",
    "\r\n",
    "<img src = \"./image/e_6_1.png\">  \r\n",
    "\r\n",
    "η(eta): learning rate / W:  가중치  \r\n",
    "위 식을 파이썬으로 구현해 보자.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\r\n",
    "    def __init__(self, lr=0.01):\r\n",
    "        self.lr = LookupError\r\n",
    "    \r\n",
    "    def update(self, params, grads):\r\n",
    "        for key in params.keys():\r\n",
    "            params[key] -=seelf.lr * grads[key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SGD 클래스를 사용하면 신경망 매개변수의 진행을 다음과 같이 실행한다.  \r\n",
    "\r\n",
    "```python\r\n",
    "\r\n",
    "network = TwoLayerNet(...)\r\n",
    "optimizer = SGD()\r\n",
    "\r\n",
    "for i in range(10000):\r\n",
    "    ...\r\n",
    "    x_batch, t_batch = get_mini_batch(...) # 미니배치\r\n",
    "    grads = network.gradient(x_batch, t_batch)\r\n",
    "    params = network.params\r\n",
    "    optimizer.update(params, grads)\r\n",
    "    ...\r\n",
    "```\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "매개변수 갱신자: optimizer  \r\n",
    "이처럼 촤적화를 담당하는 클래스를 분리해 구현하면 기능을 모듈화하기 좋음\r\n",
    "\r\n",
    "## 6.1.3 SGD의 단점  \r\n",
    "\r\n",
    "SGD는 단순하고 구현이 쉽지만 문제애 따라서 비효울적  \r\n",
    "SGD의 단점을 알아보고자 다믕 함수의 최솟값을 구해보자.  \r\n",
    "\r\n",
    "<img src = \"./image/e_6_2.png\">  \r\n",
    "\r\n",
    "이 함수를 좌표로 보면 다음과 같은 모습을 보인다.  \r\n",
    "\r\n",
    "<img src = \"./image/fig_6_1.png\" width = \"40%\">  \r\n",
    "\r\n",
    "x축 방향의 기울기는 완만하지만 y축 기울기의 방향은 가파르다.  \r\n",
    "식에서 최솟값이 되는 장소는 (x, y) = (0,0) 이지만, 아래 그림을 보면 기울기는 대부분 (0,0)을 가리키지 않고있다.  \r\n",
    "\r\n",
    "<img src = \"./image/fig_6_2.png\" width=\"40%\">  \r\n",
    "\r\n",
    "함수에 SGD를 적용해 보자. 담색의 시작 장소(초깃값)는 (x, y) = (-7.0, -2.0)으로 했다.  \r\n",
    "\r\n",
    "<img src = \"./image/fig_6_3.png\" width=\"40%\">  \r\n",
    "\r\n",
    "최솟값까지 지그재그로 이동하여 비효율적이다.  \r\n",
    "즉 SGD의 단점은 비등방성(anisotropy) 함수(방향에 따라 성질, 즉 여기에서는 기울기가 달라지는 함수)에서는 탐색 경로가 비효율적이라는 것이다.  \r\n",
    "근본적인 원인은 기울어진 방향이 본래의 최솟값과 다른 방향을 가리켜서라는 점도 생각해 봐야한다.  \r\n",
    "단점을 개선해 줄 수 있는 모멘텀, AdaGrad, Adam이라는 세 방법을 살펴보자.  \r\n",
    "\r\n",
    "## 6.1.4 모멘텀\r\n",
    "\r\n",
    "모멘텀(Momentum): 운동량을 뜻하는 단어로 물리와 관계있음, 수식으로는 다음과 같이 쓴다.  \r\n",
    "\r\n",
    "<img src = \"./image/e_6_3.png\">\r\n",
    "<img src = \"./image/e_6_4.png\">  \r\n",
    "\r\n",
    "v라는 새로운 변수는 물리에서 속도(velocity)에 해당한다.  \r\n",
    "모멘텀은 마치 공이 바닥의 기울기에 따라 구르는 듯한 움직임을 보인다.  \r\n",
    "\r\n",
    "<img src=\"./image/fig_6_4.png\">  \r\n",
    "\r\n",
    "αv 항은 물체가 아무런 힘을 받지 않으면 서서히 하강시키는 역할을 한다.  \r\n",
    "(α는 0.9등의 값으로 설정한다.) 모멘텀을 파이썬으로 구현해보자.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \r\n",
    "\r\n",
    "class Momentum:\r\n",
    "    def __init__(self, lr=0.01, momentum=0.9):\r\n",
    "        self.lr =lr\r\n",
    "        self.momentum = momentum\r\n",
    "        self.v = None\r\n",
    "\r\n",
    "    def update(self, params, grads):\r\n",
    "        if self.v is None:\r\n",
    "            self.v = {}\r\n",
    "            for key, val in params.items():\r\n",
    "                self.v[key] = np.seros_like(val)\r\n",
    "\r\n",
    "        for key in params.keys():\r\n",
    "            self.v[key] = self.momentum*self.v[key] - self.lr*grads[key]\r\n",
    "            params[key] += self.v[key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "v는 속도이다. v는 초기에 아무 값도 없지만 update() 처음 호출시 매개변수와 같은구조의 데이터를 딕셔너리 변수로 저장한다. \r\n",
    "나머지는 위의 식들을 그대로 코드로 옮겨 놓았다.  \r\n",
    "이제 모멘텀을 이용해서 SGD로 풀었던 최적화 문제를 풀어보면 다음과 같다.  \r\n",
    "\r\n",
    "<img src = \"./image/fig_6_5.png\" width=\"40%\">  \r\n",
    "\r\n",
    "아직 지그재그의 느낌이 있지만 그 폭이 줄어들고 조금더 일정하다.  \r\n",
    "\r\n",
    "## 6.1.5 AdaGrad  \r\n",
    "\r\n",
    "신경망 학습에서 학습률의 값이 중요하다.  "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ee6caf572b02fbd90c9d9a82b73a7abd542127d236c55761b702f851ee4a295f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}