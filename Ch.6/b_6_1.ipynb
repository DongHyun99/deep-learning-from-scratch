{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. 학습관련 기술들  \r\n",
    "\r\n",
    "# 6.1 매개변수 갱신  \r\n",
    "\r\n",
    "최적화(optimization): 가능한 한 손실함수가 잔은 매개변수의 최적값을 찾는 것  \r\n",
    "매개변수의 공간이 매우 넓고 복잡하여 매우 어려움  \r\n",
    "따라서 미분을 이용한 확률적 경사하강법 (SGD)를 많이 사용  \r\n",
    "앞으로는 다른 최적화 기법에 대해서도 다룰 것이다.  \r\n",
    "\r\n",
    "## 6.1.1 모험가 이야기  \r\n",
    "\r\n",
    "## 6.1.2 확률적 경사 하강법(SGD)  \r\n",
    "\r\n",
    "SGD를 수식으로 쓰면 다음과 같다.  \r\n",
    "\r\n",
    "<img src = \"./image/e_6_1.png\">  \r\n",
    "\r\n",
    "η(eta): learning rate / W:  가중치  \r\n",
    "위 식을 파이썬으로 구현해 보자.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\r\n",
    "    def __init__(self, lr=0.01):\r\n",
    "        self.lr = LookupError\r\n",
    "    \r\n",
    "    def update(self, params, grads):\r\n",
    "        for key in params.keys():\r\n",
    "            params[key] -=seelf.lr * grads[key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SGD 클래스를 사용하면 신경망 매개변수의 진행을 다음과 같이 실행한다.  \r\n",
    "\r\n",
    "```python\r\n",
    "\r\n",
    "network = TwoLayerNet(...)\r\n",
    "optimizer = SGD()\r\n",
    "\r\n",
    "for i in range(10000):\r\n",
    "    ...\r\n",
    "    x_batch, t_batch = get_mini_batch(...) # 미니배치\r\n",
    "    grads = network.gradient(x_batch, t_batch)\r\n",
    "    params = network.params\r\n",
    "    optimizer.update(params, grads)\r\n",
    "    ...\r\n",
    "```\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "매개변수 갱신자: optimizer  \r\n",
    "이처럼 촤적화를 담당하는 클래스를 분리해 구현하면 기능을 모듈화하기 좋음\r\n",
    "\r\n",
    "## 6.1.3 SGD의 단점  \r\n",
    "\r\n",
    "SGD는 단순하고 구현이 쉽지만 문제애 따라서 비효울적  \r\n",
    "SGD의 단점을 알아보고자 다믕 함수의 최솟값을 구해보자.  \r\n",
    "\r\n",
    "<img src = \"./image/e_6_2.png\">  \r\n",
    "\r\n",
    "이 함수를 좌표로 보면 다음과 같은 모습을 보인다.  \r\n",
    "\r\n",
    "<img src = \"./image/fig_6_1.png\" width = \"40%\">  \r\n",
    "\r\n",
    "x축 방향의 기울기는 완만하지만 y축 기울기의 방향은 가파르다.  \r\n",
    "식에서 최솟값이 되는 장소는 (x, y) = (0,0) 이지만, 아래 그림을 보면 기울기는 대부분 (0,0)을 가리키지 않고있다.  \r\n",
    "\r\n",
    "<img src = \"./image/fig_6_2.png\" width=\"40%\">  \r\n",
    "\r\n",
    "함수에 SGD를 적용해 보자. 담색의 시작 장소(초깃값)는 (x, y) = (-7.0, -2.0)으로 했다.  \r\n",
    "\r\n",
    "<img src = \"./image/fig_6_3.png\" width=\"40%\">  \r\n",
    "\r\n",
    "최솟값까지 지그재그로 이동하여 비효율적이다.  \r\n",
    "즉 SGD의 단점은 비등방성(anisotropy) 함수(방향에 따라 성질, 즉 여기에서는 기울기가 달라지는 함수)에서는 탐색 경로가 비효율적이라는 것이다.  \r\n",
    "근본적인 원인은 기울어진 방향이 본래의 최솟값과 다른 방향을 가리켜서라는 점도 생각해 봐야한다.  \r\n",
    "단점을 개선해 줄 수 있는 모멘텀, AdaGrad, Adam이라는 세 방법을 살펴보자.  \r\n",
    "\r\n",
    "## 6.1.4 모멘텀\r\n",
    "\r\n",
    "모멘텀(Momentum): 운동량을 뜻하는 단어로 물리와 관계있음, 수식으로는 다음과 같이 쓴다.  \r\n",
    "\r\n",
    "<img src = \"./image/e_6_3.png\">\r\n",
    "<img src = \"./image/e_6_4.png\">  \r\n",
    "\r\n",
    "v라는 새로운 변수는 물리에서 속도(velocity)에 해당한다.  \r\n",
    "모멘텀은 마치 공이 바닥의 기울기에 따라 구르는 듯한 움직임을 보인다.  \r\n",
    "\r\n",
    "<img src=\"./image/fig_6_4.png\">  \r\n",
    "\r\n",
    "αv 항은 물체가 아무런 힘을 받지 않으면 서서히 하강시키는 역할을 한다.  \r\n",
    "(α는 0.9등의 값으로 설정한다.) 모멘텀을 파이썬으로 구현해보자.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \r\n",
    "\r\n",
    "class Momentum:\r\n",
    "    def __init__(self, lr=0.01, momentum=0.9):\r\n",
    "        self.lr =lr\r\n",
    "        self.momentum = momentum\r\n",
    "        self.v = None\r\n",
    "\r\n",
    "    def update(self, params, grads):\r\n",
    "        if self.v is None:\r\n",
    "            self.v = {}\r\n",
    "            for key, val in params.items():\r\n",
    "                self.v[key] = np.seros_like(val)\r\n",
    "\r\n",
    "        for key in params.keys():\r\n",
    "            self.v[key] = self.momentum*self.v[key] - self.lr*grads[key]\r\n",
    "            params[key] += self.v[key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "v는 속도이다. v는 초기에 아무 값도 없지만 update() 처음 호출시 매개변수와 같은구조의 데이터를 딕셔너리 변수로 저장한다. \r\n",
    "나머지는 위의 식들을 그대로 코드로 옮겨 놓았다.  \r\n",
    "이제 모멘텀을 이용해서 SGD로 풀었던 최적화 문제를 풀어보면 다음과 같다.  \r\n",
    "\r\n",
    "<img src = \"./image/fig_6_5.png\" width=\"40%\">  \r\n",
    "\r\n",
    "아직 지그재그의 느낌이 있지만 그 폭이 줄어들고 조금더 일정하다.  \r\n",
    "\r\n",
    "## 6.1.5 AdaGrad  \r\n",
    "\r\n",
    "신경망 학습에서 학습률의 값이 중요하다.  \r\n",
    "학습률을 정하는 효과적인 기술로 학습률 감소(learning rate decay)가 있다.  \r\n",
    "학습을 진행하면서 점차 학습률을 줄여가는 방법이다.  \r\n",
    "원래는 학습률 값을 일괄적으로 낮췄지만 AdaGrad는 각 매개변수에 맞추어 값을 만들어준다.  \r\n",
    "갱신 방법에 대한 수식은 다음과 같다.  \r\n",
    "<img src = \"./image/e_6_5.png\"> <img src = \"./image/e_6_6.png\">  \r\n",
    "\r\n",
    "h는 기존 기울기 값을 제곱하여 계속 더해준다. (⊙는 행렬의 원소별 곱셈을 의미)  \r\n",
    "그리고 매개변수를 갱신할때 곱해 학습률을 조정한다.  \r\n",
    "(매개변수의 원소 중에서 많이 움직인 원소는 학습률이 낮아진다는 의미)  \r\n",
    "\r\n",
    "참고로 AdaGrad는 계속 갱신을 하다보면 언제인가 갱신을 멈추게 되는데 이것을 개선한 기법으로 RMSProp이 있다.  \r\n",
    "\r\n",
    "AdaGrad를 구현해 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaGrad:\r\n",
    "\r\n",
    "    def __init__(self, lr=0.01):\r\n",
    "        self.lr = lr\r\n",
    "        self.h = None\r\n",
    "        \r\n",
    "    def update(self, params, grads):\r\n",
    "        if self.h is None:\r\n",
    "            self.h = {}\r\n",
    "            for key, val in params.items():\r\n",
    "                self.h[key] = np.zeros_like(val)\r\n",
    "            \r\n",
    "        for key in params.keys():\r\n",
    "            self.h[key] += grads[key] * grads[key]\r\n",
    "            params[key] -= self.lr * grads[key] / (np.sqrt(self.h[key]) + 1e-7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "맨 마지막 줄의 1e-7은 0으로 나누는 사태를 방지하기 위해 있는 극소값이다.  \r\n",
    "최적화 문제를 풀어보면 다음과 같은 결과가 나온다.  \r\n",
    "\r\n",
    "<img src = \"./image/fig_6_6.png\" width = \"40%\">  \r\n",
    "\r\n",
    "위의 그림을 보면 최솟값을 향해 효율적으로 움직이는 것을 알수 있다.  \r\n",
    "\r\n",
    "## 6.1.6 Adam  \r\n",
    "\r\n",
    "Adam은 모멘텀과 AdaGrad를 융합하자는 생각에서 출발한 기법이다. (사실 완전히 정확하지는 않다.)  \r\n",
    "또한 Adam은 하이퍼 파라미터의 편향보정이 진행된다는 특징을 가지고 있다.  \r\n",
    "Adam을 사용해서 쵲거화 문제를 풀어보면 다음과 같은 결과가 나온다.  \r\n",
    "\r\n",
    "<img src = \"./image/fig_6_7.png\" width=\"40%\">  \r\n",
    "\r\n",
    "## 6.1.7 어느 갱신 방법을 이용할 것인가?  \r\n",
    "\r\n",
    "<img src = \"./image/fig_6_8.png\" width=\"40%\">  \r\n",
    "\r\n",
    "위 그림을 보면 사용한 기법에 따라 갱신 경로가 다르다.  \r\n",
    "보기에는 AdaGrad가 제일 나아보이지만 사실 그 결과는 폴어야 할 문제에 따라 달라짐.  \r\n",
    "또한 하이퍼 파라미터를 어떻게 설정하느냐에 따라서도 결과가 바뀐다.  \r\n",
    "모든 문제에서 항상 뛰어난 기법은 없고 각자의 장단점이 있음  \r\n",
    "\r\n",
    "## 6.1.8 MNIST 데이터 셋으로 본 갱신 방법 비교  \r\n",
    "\r\n",
    "손글씨 숫자 인식을 대상으로 지금까지의 4가지 기법을 비교해 보자.  \r\n",
    "\r\n",
    "<img src = \"./image/fig_6_9.png\" width = \"40%\">  \r\n",
    "\r\n",
    "100개의 뉴런으로 구성된 5층 신경망에서 ReLU를 활성화 함수로 사용했다.  \r\n",
    "위를 보면 SGD의 학습 진도가 가장 느리고, 나머지는 비슷한데 AdaGrad가 조금 더 빠른 것으로 보인다.  \r\n",
    "다만 하이퍼파라미터 (학습률), 신경망의 구조 등에 따라 결과는 달라질 수 있다.  \r\n",
    "일반적으론 SGD가 가장 느리고 정확도도 조금 낮다.  "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ee6caf572b02fbd90c9d9a82b73a7abd542127d236c55761b702f851ee4a295f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}