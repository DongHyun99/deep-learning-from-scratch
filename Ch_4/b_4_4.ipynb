{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "ee6caf572b02fbd90c9d9a82b73a7abd542127d236c55761b702f851ee4a295f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# 4.4 기울기  \n",
    "\n",
    "만약 x0와 x1의 편미분을 함께 계산하고싶다면 어떻게 할까?  \n",
    "모든 편미분을 묶어서 계산한다면, 이것을 벡터로 정리한 것을 기울기하고 한다. (gradient)   \n",
    "기울기를 구현해보자"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def numerical_gradient(f, x):\n",
    "    h = 1e-4 #0.0001\n",
    "    grad = np.zeros_like(x) # x와 형상이 같은 배열을 생성, 0으로 가득 차 있음\n",
    "\n",
    "    for idx in range(x.size):\n",
    "        tmp_val = x[idx]\n",
    "        \n",
    "        # f(x+h) 계산\n",
    "        x[idx] = tmp_val + h\n",
    "        fxh1 = f(x)\n",
    "\n",
    "        # f(x+h) 계산\n",
    "        x[idx] = tmp_val - h\n",
    "        fxh2 = f(x)\n",
    "\n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        x[idx] = tmp_val\n",
    "\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[6. 8.]\n[0. 4.]\n[6. 0.]\n"
     ]
    }
   ],
   "source": [
    "# 실제 기울기 계산, (3,4), (0,2), (3,0)에서의 기울기를 구해보자\n",
    "\n",
    "def function_2(x):\n",
    "    return x[0]**2 + x[1]**2 #혹은 np.sum(x**2)로도 구현 가능\n",
    "\n",
    "print(numerical_gradient(function_2, np.array([3.0,4.0])))\n",
    "print(numerical_gradient(function_2, np.array([0.0,2.0])))\n",
    "print(numerical_gradient(function_2, np.array([3.0,0.0])))"
   ]
  },
  {
   "source": [
    "편미분의 결과에 마이너스를 붙인 벡터를 그려보면 다음과 같다.  \n",
    "\n",
    "<img src = \"./image/fig_4_9.png\" width = \"40%\">  \n",
    "\n",
    "그림을 보면 기울기는 함수의 최솟값을 가르키는 것 같다.  \n",
    "사실 실제로 그렇지는 않고 각 지점에서 낮아지는 방향을 가리키는 것이다.  \n",
    "즉 기울기가 가리키는 방향은 **각 장소에서 함수의 출력을 가장 줄이는 방향**이다.\n",
    "\n",
    "## 4.4.1 경사법 (경사 하강법)  \n",
    "\n",
    "기계학습 문제와 마찬가지로 신경망 역시 최적의 매개변수(가중치와 편향)를 학습 시에 찾아야 한다. \n",
    "여기서의 최적 == 손실함수가 최솟값이 되는 매개변수의 값  \n",
    "그러나 이것을 알아내는 것은 만만치 않음  \n",
    "여기서 기울기를 이용해서 함수의 최솟값 혹은 그에 준하는 작은 값을 찾으려는 것을 경사법이라고 한다.  \n",
    "\n",
    "그러나 앞서 설명했듯이 기울기는 각 지점에서 함수의 값을 낮추는 방향이지 가르키는 방향에 정말 함수의 최솟값이 있는지는  \n",
    "알 수가 없음  (특히 복잡한 함수에서는 없는 경우가 많음)  \n",
    "\n",
    "```\n",
    "함수가 극솟갑, 최솟값, 혹은 안장점이라는 곳에서 기울기가 0다.  \n",
    "안장점은 어느 방향에서보면 극댓값이고 어느 방향에서보면 극솟값인 점이다.  \n",
    "따라서 기울기가 0인 지점이 최솟값이라고 단정할 수는 없다.  \n",
    "특히 복잡하고 찌그러진 모양의 함수는 평평한 곳으로 파고들면서 고원(plateau)이라고 하는,  \n",
    "학습이 진행되지 않는 정체기에 빠질 수 있다.  \n",
    "```\n",
    "\n",
    "다만 기울어진 방향으로 가야만 함수의 값을 줄일 수 있기 때문에 일정거리만큼 이동하면서 기울기를 구하는 과정을 반복한다.  \n",
    "이렇게 함수의 값을 조금씩 줄이는 것을 경사법이라고 한다.  \n",
    "최솟값을 찾을때는 경사하강법, 최댓값을 찾을 때는 경사 상승법을 사용하는데 어짜피 부호만 다르다.  \n",
    "다음은 경사법의 수식이다.  \n",
    "\n",
    "<img src = \"./image/e_4_7.png\">  \n",
    "\n",
    "η(eta): 갱신 량, 신경망에서는 학습률(Learning Rate)라고 부른다.  \n",
    "    한번의 학습으로 얼마만큼 학습해야 할지, 매개변수의 값을 얼마나 갱신하는지를 나타낸 값  \n",
    "\n",
    "계속해서 위의 식을 반복하면서 변수의 값을 줄여나가야 한다.  \n",
    "또한 Learning Rate의 값은 0.001, 0.001 등 미리 정해 놔야 한다.  \n",
    "적당한 값이 아닌경우 좋은 장소를 찾을 수 없다.  \n",
    "신경망 학습은 이 값을 변경해 보면서 올바르게 학습 되고 있는지를 확인하며 진행한다.  \n",
    "다음과 같이 경사하강법을 구현해 보았다."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(f, init_x, lr = 0.01, step_num = 100):\n",
    "    x = init_x\n",
    "\n",
    "    for i in range(step_num):\n",
    "        grad = numerical_gradient(f, x)\n",
    "        x -= lr * grad\n",
    "    return x"
   ]
  },
  {
   "source": [
    "    - f: 최적화 함수  \n",
    "    - int_x: 초깃값  \n",
    "    - lr: learning rate  \n",
    "    -step_num: 경사법에 따른 반복 횟수  \n",
    "이 함수를 이용해 극솟값, 더 나아가 잘하면 최솟값까지 구하는 것이 가능하다.  "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[-6.11110793e-10  8.14814391e-10]\n"
     ]
    }
   ],
   "source": [
    "# 경사법으로 f(x0, x1) = x0^2+x1^2의 최솟값을 구해보자. \n",
    "\n",
    "init_x = np.array([-3.0, 4.0])\n",
    "print(gradient_descent(function_2, init_x=init_x, lr=0.1, step_num=100))"
   ]
  },
  {
   "source": [
    "최종 결과는 [0,0]에 가까운 값이 되었다.  \n",
    "\n",
    "<img src=\"./image/fig_4_10.png\" width=\"40%\">  \n",
    "\n",
    "학습률이 너무 크거나 작으면 좋은 결과를 얻을 수 없다 했는데 실험해 보자."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[-2.58983747e+13 -1.29524862e+12]\n[-2.99999994  3.99999992]\n"
     ]
    }
   ],
   "source": [
    "# 학습률이 큰 경우: lr=10.0\n",
    "init_x = np.array([-3.0, 4.0])\n",
    "print(gradient_descent(function_2, init_x=init_x, lr=10.0, step_num=100))\n",
    "\n",
    "# 학습률이 큰 경우: lr=1e-10\n",
    "init_x = np.array([-3.0, 4.0])\n",
    "print(gradient_descent(function_2, init_x=init_x, lr=1e-10, step_num=100))"
   ]
  },
  {
   "source": [
    "lr가 너무 높으면 큰값으로 발산, 너무 작으면 거의 갱신되지 않고 끝나버린다.  \n",
    "이러한 lr과 같은 매개변수를 **하이퍼파라미터**라고 부른다.  \n",
    "즉 기계가 학습을 통해 갱신되는 매개변수가 아닌 사람이 직접줘야 하는 매개변수를 뜻한다.  \n",
    "\n",
    "## 4.4.2 신경망에서의 기울기  "
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}