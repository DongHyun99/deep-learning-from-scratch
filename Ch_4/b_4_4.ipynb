{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "ee6caf572b02fbd90c9d9a82b73a7abd542127d236c55761b702f851ee4a295f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# 4.4 기울기  \n",
    "\n",
    "만약 x0와 x1의 편미분을 함께 계산하고싶다면 어떻게 할까?  \n",
    "모든 편미분을 묶어서 계산한다면, 이것을 벡터로 정리한 것을 기울기하고 한다. (gradient)   \n",
    "기울기를 구현해보자"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def numerical_gradient(f, x):\n",
    "    h = 1e-4 #0.0001\n",
    "    grad = np.zeros_like(x) # x와 형상이 같은 배열을 생성, 0으로 가득 차 있음\n",
    "\n",
    "    for idx in range(x.size):\n",
    "        tmp_val = x[idx]\n",
    "        \n",
    "        # f(x+h) 계산\n",
    "        x[idx] = tmp_val + h\n",
    "        fxh1 = f(x)\n",
    "\n",
    "        # f(x+h) 계산\n",
    "        x[idx] = tmp_val - h\n",
    "        fxh2 = f(x)\n",
    "\n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        x[idx] = tmp_val\n",
    "\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[6. 8.]\n[0. 4.]\n[6. 0.]\n"
     ]
    }
   ],
   "source": [
    "# 실제 기울기 계산, (3,4), (0,2), (3,0)에서의 기울기를 구해보자\n",
    "\n",
    "def function_2(x):\n",
    "    return x[0]**2 + x[1]**2 #혹은 np.sum(x**2)로도 구현 가능\n",
    "\n",
    "print(numerical_gradient(function_2, np.array([3.0,4.0])))\n",
    "print(numerical_gradient(function_2, np.array([0.0,2.0])))\n",
    "print(numerical_gradient(function_2, np.array([3.0,0.0])))"
   ]
  },
  {
   "source": [
    "편미분의 결과에 마이너스를 붙인 벡터를 그려보면 다음과 같다.  \n",
    "\n",
    "<img src = \"./image/fig_4_9.png\" width = \"40%\">  \n",
    "\n",
    "그림을 보면 기울기는 함수의 최솟값을 가르키는 것 같다.  \n",
    "사실 실제로 그렇지는 않고 각 지점에서 낮아지는 방향을 가리키는 것이다.  \n",
    "즉 기울기가 가리키는 방향은 **각 장소에서 함수의 출력을 가장 줄이는 방향**이다.\n",
    "\n",
    "## 4.4.1 경사법 (경사 하강법)  \n",
    "\n",
    "기계학습 문제와 마찬가지로 신경망 역시 최적의 매개변수(가중치와 편향)를 학습 시에 찾아야 한다. \n",
    "여기서의 최적 == 손실함수가 최솟값이 되는 매개변수의 값  \n",
    "그러나 이것을 알아내는 것은 만만치 않음  \n",
    "여기서 기울기를 이용해서 함수의 최솟값 혹은 그에 준하는 작은 값을 찾으려는 것을 경사법이라고 한다.  \n",
    "\n",
    "그러나 앞서 설명했듯이 기울기는 각 지점에서 함수의 값을 낮추는 방향이지 가르키는 방향에 정말 함수의 최솟값이 있는지는  \n",
    "알 수가 없음  (특히 복잡한 함수에서는 없는 경우가 많음)  \n",
    "\n",
    "```\n",
    "함수가 극솟갑, 최솟값, 혹은 안장점이라는 곳에서 기울기가 0다.  \n",
    "안장점은 어느 방향에서보면 극댓값이고 어느 방향에서보면 극솟값인 점이다.  \n",
    "따라서 기울기가 0인 지점이 최솟값이라고 단정할 수는 없다.  \n",
    "특히 복잡하고 찌그러진 모양의 함수는 평평한 곳으로 파고들면서 고원(plateau)이라고 하는,  \n",
    "학습이 진행되지 않는 정체기에 빠질 수 있다.  \n",
    "```\n",
    "\n",
    "다만 기울어진 방향으로 가야만 함수의 값을 줄일 수 있기 때문에 일정거리만큼 이동하면서 기울기를 구하는 과정을 반복한다.  \n",
    "이렇게 함수의 값을 조금씩 줄이는 것을 경사법이라고 한다.  \n",
    "최솟값을 찾을때는 경사하강법, 최댓값을 찾을 때는 경사 상승법을 사용하는데 어짜피 부호만 다르다.  \n",
    "다음은 경사법의 수식이다.  \n",
    "\n",
    "<img src = \"./image/e_4_7.png\">  \n",
    "\n",
    "η(eta): 갱신 량, 신경망에서는 학습률(Learning Rate)라고 부른다.  \n",
    "    한번의 학습으로 얼마만큼 학습해야 할지, 매개변수의 값을 얼마나 갱신하는지를 나타낸 값  \n",
    "\n",
    "계속해서 위의 식을 반복하면서 변수의 값을 줄여나가야 한다.  \n",
    "또한 Learning Rate의 값은 0.001, 0.001 등 미리 정해 놔야 한다.  \n",
    "적당한 값이 아닌경우 좋은 장소를 찾을 수 없다.  \n",
    "신경망 학습은 이 값을 변경해 보면서 올바르게 학습 되고 있는지를 확인하며 진행한다.  \n",
    "다음과 같이 경사하강법을 구현해 보았다."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(f, init_x, lr = 0.01, step_num = 100):\n",
    "    x = init_x\n",
    "\n",
    "    for i in range(step_num):\n",
    "        grad = numerical_gradient(f, x)\n",
    "        x -= lr * grad\n",
    "    return x"
   ]
  },
  {
   "source": [
    "    - f: 최적화 함수  \n",
    "    - int_x: 초깃값  \n",
    "    - lr: learning rate  \n",
    "    -step_num: 경사법에 따른 반복 횟수  \n",
    "이 함수를 이용해 극솟값, 더 나아가 잘하면 최솟값까지 구하는 것이 가능하다.  "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[-6.11110793e-10  8.14814391e-10]\n"
     ]
    }
   ],
   "source": [
    "# 경사법으로 f(x0, x1) = x0^2+x1^2의 최솟값을 구해보자. \n",
    "\n",
    "init_x = np.array([-3.0, 4.0])\n",
    "print(gradient_descent(function_2, init_x=init_x, lr=0.1, step_num=100))"
   ]
  },
  {
   "source": [
    "최종 결과는 [0,0]에 가까운 값이 되었다.  \n",
    "\n",
    "<img src=\"./image/fig_4_10.png\" width=\"40%\">  \n",
    "\n",
    "학습률이 너무 크거나 작으면 좋은 결과를 얻을 수 없다 했는데 실험해 보자."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[-2.58983747e+13 -1.29524862e+12]\n[-2.99999994  3.99999992]\n"
     ]
    }
   ],
   "source": [
    "# 학습률이 큰 경우: lr=10.0\n",
    "init_x = np.array([-3.0, 4.0])\n",
    "print(gradient_descent(function_2, init_x=init_x, lr=10.0, step_num=100))\n",
    "\n",
    "# 학습률이 큰 경우: lr=1e-10\n",
    "init_x = np.array([-3.0, 4.0])\n",
    "print(gradient_descent(function_2, init_x=init_x, lr=1e-10, step_num=100))"
   ]
  },
  {
   "source": [
    "lr가 너무 높으면 큰값으로 발산, 너무 작으면 거의 갱신되지 않고 끝나버린다.  \n",
    "이러한 lr과 같은 매개변수를 **하이퍼파라미터**라고 부른다.  \n",
    "즉 기계가 학습을 통해 갱신되는 매개변수가 아닌 사람이 직접줘야 하는 매개변수를 뜻한다.  \n",
    "\n",
    "## 4.4.2 신경망에서의 기울기  \n",
    "\n",
    "신경망에서의 기울기: 가중치 매개변수에 대한 손실함수의 기울기  \n",
    "가중치가 W, 손실함수가 L인 신경망에대한 경사는 다음과 같다.  \n",
    "\n",
    "<img src = \"./image/e_4_8.png\" width = \"20%\">  \n",
    "\n",
    "두번째 식의 각 원소는 각각의 원소에 관한 편미분이다.  \n",
    "즉 가중치 w에 대해서 그 w를 조금 변경했을 때 손실함수 L이 얼마나 변화 하느냐를 나타낸다.  \n",
    "따라서 가중치의 형태 (metrics)만큼에 대한 경사가 존재한다. (W와 형태가 같다.)  "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "from common.functions import softmax, cross_entropy_error\n",
    "from common.gradient  import numerical_gradient\n",
    "\n",
    "class simpleNet:\n",
    "    def __init__(self):\n",
    "        self.W = np.random.randn(2,3) # 정규분포로 초기화\n",
    "\n",
    "    def predict(self, x):\n",
    "        return np.dot(x, self.W)\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        z = self.predict(x)\n",
    "        y = softmax(z)\n",
    "        loss = cross_entropy_error(y, t)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "source": [
    "복습해보면  \n",
    "\n",
    "    - softmax: 출력층의 활성화 함수로 분류 문제에서 주로 사용됨  \n",
    "    - cross_entry_error: 교차 엔트로피 오차 손실함수로 분류 문제에서 주로 사용됨  \n",
    "    - 기울기를 구하는 편미분 함수  "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[ 1.51959983 -0.10425575 -0.75642112]\n [-1.17692132  0.9014997   1.18206675]]\n[-0.14746929  0.74879628  0.6100074 ]\n최대값 인덱스: 1\n손실 함수: 0.9623070755902571\n"
     ]
    }
   ],
   "source": [
    "net = simpleNet()\n",
    "print(net.W)\n",
    "\n",
    "x = np.array([0.6, 0.9])\n",
    "p = net.predict(x)\n",
    "print(p)\n",
    "\n",
    "print(\"최대값 인덱스:\",np.argmax(p)) # 최대값의 인덱스\n",
    "\n",
    "t = np.array([0,0,1]) # 정답 레이블\n",
    "print(\"손실 함수:\",net.loss(x, t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[ 0.10746287  0.26333077 -0.37079364]\n [ 0.1611943   0.39499616 -0.55619046]]\n"
     ]
    }
   ],
   "source": [
    "def f(W): # W는 dummy \n",
    "    return net.loss(x, t)\n",
    "\n",
    "dW = numerical_gradient(f, net.W)\n",
    "print(dW)"
   ]
  },
  {
   "source": [
    "위 코드에서는 net.W를 인수로 받아 손실함수를 계산하는 새로운 함수 f를 정의  \n",
    "그리고 편미분 함수에 넣어서 경사를 계산했다.  \n",
    "dw의 값은 2 * 3의 배열로 이루어져있는데 여기서 h를 늘리면 dw * h만큼 손실함수의 값이 증가한다는 의미가 될 것이다.  \n",
    "따라서 손실 함수를 줄인다는 측면 -> 양수인 dw의 값은 경사가 증가하므로 반대방향으로 진행시켜야 하고  \n",
    "음수인 dw의 값은 경사가 감소하므로 가능 방향으로 진행 시키면 손실함수가 감소 할 것이다.  \n",
    "\n",
    "신경망의 기울기를 구한 후에는 경사법에 따라 가중치 매개변수를 갱신하면 된다.  \n",
    "\n",
    "참고로 위와같은 간단한 함수는 lanbda를 이용하는 것이 더 편하다.  "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[ 0.10746287,  0.26333077, -0.37079364],\n",
       "       [ 0.1611943 ,  0.39499616, -0.55619046]])"
      ]
     },
     "metadata": {},
     "execution_count": 60
    }
   ],
   "source": [
    "f = lambda w: net.loss(x,t)\n",
    "dw = numerical_gradient(f,net.W)\n",
    "dw"
   ]
  }
 ]
}